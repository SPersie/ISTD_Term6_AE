Order[i,] <- c(V, rep(NA, times=length(spl1)-length(V)))
}
# Let's check the results
length(Cor)
dim(Order)
Order[,1:20]
Order[,500:520]
Order[,671:691]
#
# Now, we compute user predictions by looking at the 250 nearest neighbours
# and averaging equally over all these user ratings in the items in spl2c
for(i in 1:length(spl1c))
{UserPred[i,] <- colMeans(Data[spl1[Order[i,1:250]],spl2c], na.rm=TRUE)}
# Calculate the model error
RMSEBase1    <- sqrt(mean((Data[spl1c,spl2c] - Base1)^2, na.rm=TRUE)) # 0.9310296
RMSEBase2    <- sqrt(mean((Data[spl1c,spl2c] - Base2)^2, na.rm= TRUE)) # 0.9953377
RMSEUserPred <- sqrt(mean((Data[spl1c,spl2c] - UserPred)^2,na.rm=TRUE)) # 0.8985567
# Last step: we vary the neighborhood set for the third model to see whether it positively affects the results
RMSE <- rep(NA, times=490)
for(k in 10:499)
{for(i in 1:length(spl1c))
{UserPred[i,] <- colMeans(Data[spl1[Order[i,1:k]],spl2c], na.rm=TRUE)}
RMSE[k-10] <- sqrt(mean((Data[spl1c,spl2c] - UserPred)^2,na.rm=TRUE))}
plot(10:499,RMSE)
abline(h=0.931)
setwd("~/Desktop/AE/Week11")
# Remove all variables from the R environment to create a fresh start
rm(list=ls())
# Install the "jpeg" package to read, write, and display images in "jpeg" format
if(!require(jpeg)){
install.packages("jpeg")
library(jpeg)
}
# Define matrix
A <- matrix(c(2, 2, 3, 1), nrow=2, ncol=2)
# Eigenvalues and eigenvectors
A_eig <- eigen(A)
#
# Implement the eigendecomposition (note the operator %*% for the matrix multiplication)
A_eig$vectors %*% diag(A_eig$values) %*% solve(A_eig$vectors)
A - (A_eig$vectors %*% diag(A_eig$values) %*% solve(A_eig$vectors))
# Define psd matrix
A <- matrix(c(3, 1, 1, 3), nrow=2, ncol=2)
# A %*% t(A) == t(A) %*% A
# Eigenvalues and eigenvectors
A_eig <- eigen(A)
#
# Implement the eigendecomposition
A_eig$vectors %*% diag(A_eig$values) %*% solve(A_eig$vectors)
A_eig$vectors %*% diag(A_eig$values) %*% t(A_eig$vectors)
A - (A_eig$vectors %*% diag(A_eig$values) %*% t(A_eig$vectors))
A_eig
# Define matrix
A <- matrix(c(2, 2, 3, 1), nrow=2, ncol=2)
# Eigenvalues and eigenvectors
A_eig <- eigen(A)
A_eig$vectors
#
# Implement the eigendecomposition (note the operator %*% for the matrix multiplication)
A_eig$vectors %*% diag(A_eig$values) %*% solve(A_eig$vectors)
A_eig$vectors
?SOLVE
?solve
A - (A_eig$vectors %*% diag(A_eig$values) %*% solve(A_eig$vectors))
#
# Implement the eigendecomposition (note the operator %*% for the matrix multiplication)
A_eig$vectors %*% diag(A_eig$values) %*% solve(A_eig$vectors)
A - (A_eig$vectors %*% diag(A_eig$values) %*% solve(A_eig$vectors))
# Define psd matrix
A <- matrix(c(3, 1, 1, 3), nrow=2, ncol=2)
# A %*% t(A) == t(A) %*% A
# Eigenvalues and eigenvectors
A_eig <- eigen(A)
#
# Implement the eigendecomposition
A_eig$vectors %*% diag(A_eig$values) %*% solve(A_eig$vectors)
A_eig$vectors %*% diag(A_eig$values) %*% t(A_eig$vectors)
A - (A_eig$vectors %*% diag(A_eig$values) %*% t(A_eig$vectors))
# Remove data
rm(A,A_eig)
# Define matrix
X <- matrix(c(2,1,5,7,0,0,6,0,0,10,8,0,7,8,0,6,1,4,5,0), nrow=5, ncol=4)
#
# Apply SVD
s <- svd(X)
#
# Read output
s$u # left singular vectors
s$d # singular values
s$v # right singular vectors
#
# Test
s$u %*% diag(s$d) %*% t(s$v)
X - (s$u %*% diag(s$d) %*% t(s$v))
# Approximating X
X_hat <- s$u[,1:2] %*% diag(s$d[1:2]) %*% t(s$v[,1:2])
X - X_hat
# Calculate the explained variance
var <- cumsum(s$d^2)
str(var)
plot(1:4,var/max(var))
# We start by reading the lky image as a 3D array.
?readJPEG
lky <- readJPEG("lky.jpg")
str(lky)
# The figure has a size of 6 Mb. This is read into an array of size 410 x 640 x 3,
# height x width x channel.
#
# The values of the channels vary between 0.313 to 1.
min(lky[,,1])
max(lky[,,1])
#
# We can verify that all three channels have the same values
max(abs((lky[,,1]-lky[,,2])))
max(abs((lky[,,1]-lky[,,3])))
# Let's now perform the Singular Value Decomposition (SVD)
s <- svd(lky[,,1])
s$d # singular values
s$u # left singular vectors
s$v # right singular vectors
# With s, we can now perform a low rank approximation. In particular,
# we do a rank-10 approximation by using the top 10 singular values.
lky10 <- s$u[,1:10]%*%diag(s$d[1:10])%*%t(s$v[,1:10])
#
# Save the results into a jpeg file
?writeJPEG
writeJPEG(lky10,"lky10.jpg") # Note that this writes a grayscale image of size 16kb.
# ... obviosuly, the file is lossy and loses a lot of information.
#
# Let's retry with a rank50 approximation.
lky50 <- s$u[,1:50]%*%diag(s$d[1:50])%*%t(s$v[,1:50])
writeJPEG(lky50,"lky50.jpg")
# Let's plot the cumulative sum of the (squared) singular values, so as to calculate the explained variance
var <- cumsum(s$d^2)
str(var)
plot(1:410,var/max(var))
# The plot shows that with a few singular values we can get ~99% of the explain variance. How many?
min(which(var/max(var)>0.99)) # --> 18
# We now repeat the analysis using a RGB image
pansy <- readJPEG("pansy.jpg")
str(pansy)
# The figure has a size of 6.4 Mb. This is read into an array of size 600 x 465 x 3,
# height x width x channel (with each channel representing the RGB intensity.
# Note that in this case pansy[,,1] is different than pansy[,,2]
max(abs((pansy[,,1]-pansy[,,2])))
# Let's now perform the Singular Value Decomposition (SVD) on the different channels
s1 <- svd(pansy[,,1])
s2 <- svd(pansy[,,2])
s3 <- svd(pansy[,,3])
#
# Develop low rank approximation (with rank 50)
# initialize the vector for storing the approximation
pansy50 <- array(dim=c(600,465,3))
pansy50[,,1] <- s1$u[,1:50]%*%diag(s1$d[1:50])%*%t(s1$v[,1:50])
pansy50[,,2] <- s2$u[,1:50]%*%diag(s2$d[1:50])%*%t(s2$v[,1:50])
pansy50[,,3] <- s3$u[,1:50]%*%diag(s3$d[1:50])%*%t(s3$v[,1:50])
#
# Save the image in a jpeg file of ize ~40 kb, which has some blurring edges of the flower and leaves.
writeJPEG(pansy50,"pansy50.jpg")
#
# let's retey with a higher approximation
pansy350 <- array(dim=c(600,465,3))
pansy350[,,1] <- s1$u[,1:350]%*%diag(s1$d[1:350])%*%t(s1$v[,1:350])
pansy350[,,2] <- s2$u[,1:350]%*%diag(s2$d[1:350])%*%t(s2$v[,1:350])
pansy350[,,3] <- s3$u[,1:350]%*%diag(s3$d[1:350])%*%t(s3$v[,1:350])
writeJPEG(pansy350,"pansy350.jpg") # --> much better!
# Define matrix
X <- matrix(c(2,1,5,7,0,0,6,0,0,10,8,0,7,8,0,6,1,4,5,0), nrow=5, ncol=4)
#
# Apply SVD
s <- svd(X)
#
# Read output
s$u # left singular vectors
s$d # singular values
s$v # right singular vectors
#
# Test
s$u %*% diag(s$d) %*% t(s$v)
X - (s$u %*% diag(s$d) %*% t(s$v))
s$d # singular values
# Approximating X
X_hat <- s$u[,1:2] %*% diag(s$d[1:2]) %*% t(s$v[,1:2])
X - X_hat
# Calculate the explained variance
var <- cumsum(s$d^2)
str(var)
plot(1:4,var/max(var))
17.8999659^2
# Remove data
rm(X, X_hat, s, var)
# Let's plot the cumulative sum of the (squared) singular values, so as to calculate the explained variance
var <- cumsum(s$d^2)
lky <- readJPEG("lky.jpg")
str(lky)
# The figure has a size of 6 Mb. This is read into an array of size 410 x 640 x 3,
# height x width x channel.
#
# The values of the channels vary between 0.313 to 1.
min(lky[,,1])
max(lky[,,1])
#
# We can verify that all three channels have the same values
max(abs((lky[,,1]-lky[,,2])))
max(abs((lky[,,1]-lky[,,3])))
# Let's now perform the Singular Value Decomposition (SVD)
s <- svd(lky[,,1])
s$d # singular values
s$u # left singular vectors
s$v # right singular vectors
# With s, we can now perform a low rank approximation. In particular,
# we do a rank-10 approximation by using the top 10 singular values.
lky10 <- s$u[,1:10]%*%diag(s$d[1:10])%*%t(s$v[,1:10])
#
# Save the results into a jpeg file
?writeJPEG
# ... obviosuly, the file is lossy and loses a lot of information.
#
# Let's retry with a rank50 approximation.
lky50 <- s$u[,1:50]%*%diag(s$d[1:50])%*%t(s$v[,1:50])
# Let's plot the cumulative sum of the (squared) singular values, so as to calculate the explained variance
var <- cumsum(s$d^2)
str(var)
plot(1:410,var/max(var))
# The plot shows that with a few singular values we can get ~99% of the explain variance. How many?
min(which(var/max(var)>0.99)) # --> 18
# Define matrix
X <- matrix(c(2,1,5,7,0,0,6,0,0,10,8,0,7,8,0,6,1,4,5,0), nrow=5, ncol=4)
#
# Apply SVD
s <- svd(X)
#
# Read output
s$u # left singular vectors
s$d # singular values
s$v # right singular vectors
#
# Test
s$u %*% diag(s$d) %*% t(s$v)
X - (s$u %*% diag(s$d) %*% t(s$v))
# Approximating X
X_hat <- s$u[,1:2] %*% diag(s$d[1:2]) %*% t(s$v[,1:2])
X - X_hat
# Calculate the explained variance
var <- cumsum(s$d^2)
str(var)
s$d # singular values
# Let's plot the cumulative sum of the (squared) singular values, so as to calculate the explained variance
var <- cumsum(s$d^2)
# Remove data
rm(X, X_hat, s, var)
# We start by reading the lky image as a 3D array.
?readJPEG
lky <- readJPEG("lky.jpg")
str(lky)
# The figure has a size of 6 Mb. This is read into an array of size 410 x 640 x 3,
# height x width x channel.
#
# The values of the channels vary between 0.313 to 1.
min(lky[,,1])
max(lky[,,1])
#
# We can verify that all three channels have the same values
max(abs((lky[,,1]-lky[,,2])))
max(abs((lky[,,1]-lky[,,3])))
# Let's now perform the Singular Value Decomposition (SVD)
s <- svd(lky[,,1])
s$d # singular values
s$u # left singular vectors
s$v # right singular vectors
# With s, we can now perform a low rank approximation. In particular,
# we do a rank-10 approximation by using the top 10 singular values.
lky10 <- s$u[,1:10]%*%diag(s$d[1:10])%*%t(s$v[,1:10])
#
# Save the results into a jpeg file
?writeJPEG
writeJPEG(lky10,"lky10.jpg") # Note that this writes a grayscale image of size 16kb.
# ... obviosuly, the file is lossy and loses a lot of information.
#
# Let's retry with a rank50 approximation.
lky50 <- s$u[,1:50]%*%diag(s$d[1:50])%*%t(s$v[,1:50])
writeJPEG(lky50,"lky50.jpg")
# Let's plot the cumulative sum of the (squared) singular values, so as to calculate the explained variance
var <- cumsum(s$d^2)
str(var)
plot(1:410,var/max(var))
# The plot shows that with a few singular values we can get ~99% of the explain variance. How many?
min(which(var/max(var)>0.99)) # --> 18
# We now repeat the analysis using a RGB image
pansy <- readJPEG("pansy.jpg")
str(pansy)
# The figure has a size of 6.4 Mb. This is read into an array of size 600 x 465 x 3,
# height x width x channel (with each channel representing the RGB intensity.
# Note that in this case pansy[,,1] is different than pansy[,,2]
max(abs((pansy[,,1]-pansy[,,2])))
# Let's now perform the Singular Value Decomposition (SVD) on the different channels
s1 <- svd(pansy[,,1])
s2 <- svd(pansy[,,2])
s3 <- svd(pansy[,,3])
#
# Develop low rank approximation (with rank 50)
# initialize the vector for storing the approximation
pansy50 <- array(dim=c(600,465,3))
pansy50[,,1] <- s1$u[,1:50]%*%diag(s1$d[1:50])%*%t(s1$v[,1:50])
pansy50[,,2] <- s2$u[,1:50]%*%diag(s2$d[1:50])%*%t(s2$v[,1:50])
pansy50[,,3] <- s3$u[,1:50]%*%diag(s3$d[1:50])%*%t(s3$v[,1:50])
#
# Save the image in a jpeg file of ize ~40 kb, which has some blurring edges of the flower and leaves.
writeJPEG(pansy50,"pansy50.jpg")
#
# let's retey with a higher approximation
pansy350 <- array(dim=c(600,465,3))
pansy350[,,1] <- s1$u[,1:350]%*%diag(s1$d[1:350])%*%t(s1$v[,1:350])
pansy350[,,2] <- s2$u[,1:350]%*%diag(s2$d[1:350])%*%t(s2$v[,1:350])
pansy350[,,3] <- s3$u[,1:350]%*%diag(s3$d[1:350])%*%t(s3$v[,1:350])
writeJPEG(pansy350,"pansy350.jpg") # --> much better!
# Remove all variables from the R environment to create a fresh start
rm(list=ls())
# Install the "survival" package, which includes functions for running the Tobit model
if(!require(survival)){
install.packages("survival")
library(survival)
}
# Read the "extramarital.csv" file
exm <- read.csv("extramarital.csv")
str(exm)
# Let's take a better look at the data
table(exm$time_in_affairs>0)
# FALSE  TRUE
# 4313  2053
# About 32% of the people are in an affair
#
# Let's look at the distribution of time_in_affairs
hist(exm$time_in_affairs,100)
# Create training and test dataset
set.seed(100)
spl <- sample(nrow(exm),0.7*nrow(exm))
train <- exm[spl,]
test <- exm[-spl,]
str(train)
str(test)
# Fit the Tobit model
?Surv     # Create a survival object, usually used as a response variable in a model formula.
?survreg  # Fit a parametric survival regression model.
model1 <- survreg(Surv(time_in_affairs,time_in_affairs>0,type="left")~.,data=train,dist="gaussian")
# This fits a Tobit model where the data are left censored at zero. The error terms are assumed to be gaussian.
summary(model1)
# Let's take a better look at the data
table(exm$time_in_affairs>0)
# FALSE  TRUE
# 4313  2053
# About 32% of the people are in an affair
#
# Let's look at the distribution of time_in_affairs
hist(exm$time_in_affairs,100)
# Create training and test dataset
set.seed(100)
spl <- sample(nrow(exm),0.7*nrow(exm))
train <- exm[spl,]
test <- exm[-spl,]
str(train)
str(test)
model1 <- survreg(Surv(time_in_affairs,time_in_affairs>0,type="left")~.,data=train,dist="gaussian")
# This fits a Tobit model where the data are left censored at zero. The error terms are assumed to be gaussian.
summary(model1)
# Let's look at the parameters:
# marriage_rating: negative, meaning that as the marriage rating increases, the chances/length of an affair decrease.
# occupation: positive, meaning that there are greater chances of an affair with a better occupation.
# yrs_married: positive
# religiosity: negative
# Also, note that all coefficients are significant. The model has a log likelihhod of -5436.9.
#
# Prediction
predict1 <- predict(model1,newdata=test) # Prediction provides the latent predicted values of the variables (positive and negative)
plot(predict1)
# Benchmark model: linear regression
model2 <- lm(time_in_affairs~.,data=train)
summary(model2)
predict2 <- predict(model2,newdata=test)
# Let's compare the two models
table(predict1 <= 0, test$time_in_affairs==0)
#       FALSE TRUE
# FALSE    92   43
# TRUE    523 1252
#
table(predict2 <= 0, test$time_in_affairs==0)
#       FALSE  TRUE
#       FALSE  TRUE
# FALSE   582 1175
#       FALSE  TRUE
# FALSE   582 1175
# TRUE    33   120
#       FALSE  TRUE
# FALSE   582 1175
# TRUE    33   120
#
# Remove all variables from the R environment to create a fresh start
rm(list=ls())
# Load the "survival" package
if(!require(survival)){
install.packages("survival")
library(survival)
}
# heart.csv data (These data come from the Stanford Heart Transplantation Program)
#
# The goal is to estimate the survival of patients from the data and understand the effect that other explanatory variables may have
# (on whether transplantation helps). Note that in some cases the appropriate heart for transplant might note be available, and patients
# need to wait for it. This study was conducted in April 1, 1974. The survival time is censored if the patient drops out of program
# (no follow up information) or the patient is alive at the time if the end of the study.
#
# We start by reading the "heart.csv" file
heart <- read.csv("heart.csv")
str(heart)
# 172 observations for 7 variables:
# start, stop: entry and exit times for this time interval (in days)
# event: 1 = dead, 0 = alive
# age: age at the start of the programme
# surgery: prior bypass surgery (1 = yes, 0 = no)
# transplant: received transplant (1 = yes, 0 = no)
# id: patient ID
#
unique(heart$id) # so we have a total of 103 patients
#
# Let's take a look at some specific patients:
subset(heart,id==1)   # Patient 1 died at the age of 31 without going through a transplant
subset(heart,id==4)   # Patient 4 waited 36 days for a transplant. He/she then died after 3 days.
subset(heart,id==25)  # Patient 25 was alive at the end of the programme.
subset(heart,id==102) # patient 102 dropped the program after 11 days
# Remove all variables from the R environment to create a fresh start
rm(list=ls())
# Load the "survival" package
if(!require(survival)){
install.packages("survival")
library(survival)
}
# heart.csv data (These data come from the Stanford Heart Transplantation Program)
#
# The goal is to estimate the survival of patients from the data and understand the effect that other explanatory variables may have
# (on whether transplantation helps). Note that in some cases the appropriate heart for transplant might note be available, and patients
# need to wait for it. This study was conducted in April 1, 1974. The survival time is censored if the patient drops out of program
# (no follow up information) or the patient is alive at the time if the end of the study.
#
# We start by reading the "heart.csv" file
heart <- read.csv("heart.csv")
str(heart)
# 172 observations for 7 variables:
# start, stop: entry and exit times for this time interval (in days)
# event: 1 = dead, 0 = alive
# age: age at the start of the programme
# surgery: prior bypass surgery (1 = yes, 0 = no)
# transplant: received transplant (1 = yes, 0 = no)
# id: patient ID
#
unique(heart$id) # so we have a total of 103 patients
#
# Let's take a look at some specific patients:
subset(heart,id==1)   # Patient 1 died at the age of 31 without going through a transplant
subset(heart,id==4)   # Patient 4 waited 36 days for a transplant. He/she then died after 3 days.
subset(heart,id==25)  # Patient 25 was alive at the end of the programme.
subset(heart,id==102) # patient 102 dropped the program after 11 days
# Let's check the "survfit" and "Surv" functions
?Surv    # Create a survival object, usually used as a response variable in a model formula.
?survfit # Create survival curves
# Estimate the survival function and store it in the variable "km"
km <- survfit(Surv(start,stop,event)~1,data=heart)
summary(km,censored=TRUE) # summary of the model, with patients' survival probability
# plot the Kaplan-Meier curve along with 95% confidence interval
plot(km)
# let's take a detailed look at the results
subset(heart,stop==1)
subset(heart,stop==2)
summary(km,censored=TRUE) # summary of the model, with patients' survival probability
# let's take a detailed look at the results
subset(heart,stop==1)
subset(heart,stop==2)
# Check "coxph" function
?coxph
# Modelling
cox <- coxph(Surv(start,stop,event)~age+surgery+transplant,data=heart)
summary(cox)
# Visual analysis
plot(survfit(cox))
summary(survfit(cox))
setwd("~/Desktop/AE/Week8")
setwd("~/Desktop/AE/Week8/Prac")
library(rpart)
library(rpart.plot)
census <- read.csv("census.csv")
set.seed(2000)
spl <- sample.split(census$over50k, SplitRatio = 0.6)
train <- subset(census, spl == TRUE)
test <- subset(census, spl == FALSE)
tree1 <- rpart(over50k ~., data = train)
prp(tree1)
summary(tree1)
pred.tree1 <- predict(tree1, newdata = test, type = "prob")
summary(pred.tree1)
caTools
library(caTools)
?caTools
library(SnowballC)
library(e1071)
library(flexclust)
install.packages(flexclust)
install.packages('flexclust')
?tapply
q <- matrix(0,nrow=460,ncol=1)
dim(q)
q
q[23] <- 1
dim(q)
hatq <- solve(diag(m$d[1:2]))%*%t(m$u[,1:2])%*%q
